# ğŸ§  Self-Improving Multimodal AI
**Reinforcement Learningâ€“powered Visionâ€“Language Agent with Memory-Augmented Retrieval**

This project implements a **self-improving multimodal AI system** that answers visual questions about images with **high accuracy, low latency, and improved grounding**.  
It combines three core components:  

- ğŸ–¼ï¸ **Visionâ€“Language Model (InstructBLIP Flan-T5-XL)** â€“ imageâ€“text grounding.  
- ğŸ“š **Memory-Augmented Retrieval (MAR)** â€“ recalls similar solved cases to improve consistency.  
- ğŸ¯ **Reinforcement Learning (PPO Controller)** â€“ learns when to use memory vs. answer directly, optimising the trade-off between **accuracy, faithfulness, and latency**.  

---

## ğŸš€ Key Features
- **Adaptive Retrieval** â€“ dynamically decides per question whether to consult memory.  
- **Accuracy & Speed** â€“ achieves **81.4% Strict accuracy, 75.6% VQA2 accuracy** within ~392 ms latency.  
- **Faithfulness** â€“ selectively reuses memory while avoiding distractors, improving grounding of answers.  
- **Deployment Ready** â€“ distilled lightweight gate for real-time applications.  
- **Interactive Demo** â€“ try it live on [Hugging Face Spaces](https://huggingface.co/spaces/cd2412/VQA_Memory__RL_Controller).  

---

## ğŸ“Š Results (Validation on 1,000 items)
| Method        | Strict Accuracy | VQA2 Accuracy | Mean Latency | p95 Latency |
|---------------|----------------|---------------|--------------|-------------|
| Direct-only   | 81.10%         | 75.37%        | ~250 ms      | ~400 ms     |
| MAR-always    | 79.40%         | 74.13%        | ~650 ms      | ~1400 ms    |
| **PPO (final)** | **81.40%**     | **75.60%**    | **392 ms**   | **1080 ms** |

---

## âš™ï¸ Tech Stack
- **Python** (3.10+)  
- **PyTorch** â€“ deep learning framework  
- **Hugging Face Transformers** â€“ visionâ€“language backbone  
- **FAISS** â€“ memory-augmented retrieval  
- **Stable-Baselines3 (PPO)** â€“ reinforcement learning controller  
- **Gradio** â€“ interactive web demo  
- **NumPy / Pandas / Matplotlib** â€“ data handling & analysis  

---

## ğŸ“‚ Repository Structure
self-improving-multimodal-ai/
â”‚â”€â”€ notebooks/ # Training & evaluation notebooks
â”‚â”€â”€ models/ # Saved PPO checkpoints & distilled models
â”‚â”€â”€ data/ # Splits & configs (no raw datasets)
â”‚â”€â”€ src/ # Core pipeline (encoder, retriever, gate, generator)
â”‚â”€â”€ demo/ # Gradio app for interactive demo
â”‚â”€â”€ README.md # Project documentation


---

## â–¶ï¸ How to Run
1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/self-improving-multimodal-ai.git
   cd self-improving-multimodal-ai
   
2. Install dependencies:
   pip install -r requirements.txt
   
4. Launch demo locally:
   python app.py

5.Or try it directly on Hugging Face Spaces

## ğŸ“Œ Use Cases
Product Support AI â€“ answering customer queries about products from images.

Education & Tutoring â€“ assisting with visual reasoning tasks.

Healthcare Support â€“ retrieval-augmented reasoning over medical imagery (future work).

Interactive Assistants â€“ grounding responses with both images and context.

## âœ¨ Contributions & Future Work

Add spatial priors for â€œwhereâ€ questions.

Improve faithfulness logging with large-scale attribution checks.

Extend to multi-turn dialogues and domain-specific datasets.

Explore cost-aware reward shaping for production.

## ğŸ‘¨â€ğŸ’» Author

Chaitanya Rajesh Deokar
MSc in Computing (Data Science) â€“ Technological University Dublin
